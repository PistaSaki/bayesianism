{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{P}{\\mathbb{P}}$\n",
    "$\\newcommand{R}{\\mathbb{R}}$\n",
    "$\\newcommand{Z}{\\mathbb{Z}}$\n",
    "$\\newcommand{E}{\\mathbb{E}}$\n",
    "# The problem\n",
    "\n",
    "The typical statistical problem comprises random variables\n",
    "* $\\theta$ - the unknown model variables we are interested in.\n",
    "* $Y$ - the unknown latent variables we don't care about. \n",
    "Usualy the dimension increases with number of observations i.e. we have $Y_i$; $i \\in \\{1, \\ldots, n\\}$.\n",
    "* $Z$ - observations. Usualy composed of independent parts $Z_i$.\n",
    "\n",
    "Moreover we (pretend to) know the following probas:\n",
    "* $\\P(\\theta)$ - prior on model parameters\n",
    "* $\\P(Y\\mid \\theta)$ - \"prior\" on latent variables. Usualy decomposes as $\\P(Y\\mid\\theta) = \\prod_i\\P(Y_i\\mid\\theta)$.\n",
    "* $\\P(Z\\mid Y,\\ \\theta)$ - \"likelihood\". Usualy $\\P(Z\\mid Y,\\ \\theta) = \\prod_i \\P(Z_i\\mid Y_i,\\ \\theta)$\n",
    "\n",
    "**Remark:**\n",
    "If you are a frequentist then you can usualy take (improper) prior $\\P(\\theta) = 1$; $\\P(Y\\mid \\theta)= 1$ and change the wording a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "### Example from  Andel\n",
    "### Sport Models in Tipsport\n",
    "### Dimensionality reduction\n",
    "### Clustering\n",
    "### Hidden Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution \n",
    "There are two groups of variables: the observed stuff $Z$ and the unobserved stuff $\\theta, Y$. Using standard Bayesian reasoning we get the posterior on unobserved stuff i.e. a joint $(y, \\theta)$-posterior:\n",
    "$$ \\P(y, \\theta\\mid z) \\propto \\P(\\theta) \\P(y\\mid \\theta) \\cdot \\P(z\\mid \\theta, y)$$\n",
    "What do we expect as a solution?\n",
    "* Maximal joint likelihood/posterior\n",
    "* Full Bayesian solution\n",
    "* something in between: maximal $\\theta$-posterior\n",
    "\n",
    "#### Maximal joint posterior\n",
    "$\\newcommand{\\argmax}[1]{\\underset{#1}{\\mathrm{argmax}}}$\n",
    "You calculate \n",
    "$$\\hat\\theta, \\hat y = \\argmax{\\theta, y}\\; \\P(y, \\theta \\mid z)$$\n",
    "and keep only $\\hat\\theta$. This is not allways satisfactory - the Andel example.\n",
    "\n",
    "#### Full Bayesian solution\n",
    "You can sample both $(y, \\theta)$ from the joint posterior\n",
    "\n",
    "and then keep only the samle of thetas. This is equivalent to sampling thetas from the posterior \n",
    "$$\\P(\\theta\\mid z) = \\sum_y \\P(y, \\theta \\mid z).$$\n",
    "\n",
    "The practical problem is what to do with this sample? \n",
    "\n",
    "**Example**\n",
    "* $\\theta = (\\theta_1, \\theta_2) \\in \\R^2$ drawn from uniform prior\n",
    "* $Y_i \\sim \\mathrm{Bernoulli}(p = \\frac 1 2)  \\in \\{0, 1\\}$ \n",
    "* $Z_i \\sim \\mathrm{Gauss}( \\mathrm{mean} = \\theta_{Y_i}, \\mathrm{var} = 1)$.\n",
    "\n",
    "Imagine that the true $\\theta = (-1, 1)$ and your sample is large enough. Notice that the model is invariant w.r.t. the action of the cyclic group $(\\Z_2, +)$ given by \n",
    "$$\\theta_1, \\theta_2 \\mapsto \\theta_2, \\theta_1.$$ \n",
    "Thus the $\\theta$-posterior will be symmetric w.r.t. the same group and will have two peaks at (-1, 1) and (1, -1). \n",
    "In particular the mean of the posterior is $\\theta = (0, 0)$ that does not give a very good model.\n",
    "Taking one of the maximums would be the good thing to do.\n",
    "\n",
    "**Remark:** A hardcore Bayesian would not try to replace the distribution of $\\theta$ by one value.\n",
    "\n",
    "#### Something in between: Maximal $\\theta$-posterior\n",
    "$$\\hat\\theta = \\argmax{\\theta}\\; \\P(\\theta\\mid z)= \\argmax{\\theta} \\sum_y\\P(\\theta, y\\mid z) $$\n",
    "This is usualy the right thing to do. The following section is about how to calculate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EM algorithm\n",
    "#### The algorithm repeats two steps:\n",
    "* **E-step:** for fixed $\\tilde\\theta$ calculate the \"posterior density of $y$ given $z$, $\\theta$\":\n",
    "$$\\tilde P(y) := \\P(y\\mid \\tilde\\theta, z) \\propto \\P(y\\mid \\tilde\\theta) \\cdot \\P(z\\mid \\tilde\\theta, y)$$\n",
    "* **M-step:** Now fix $\\tilde P$ and maximize\n",
    "$$\\tilde\\theta = \\argmax{\\theta}\\; \\E_{\\tilde P}\\Big[\\log\\P(z, y, \\theta) \\Big]$$\n",
    "(Expressing it using the given functions: $\\P(z, y,\\theta) = \\P(z\\mid \\theta, y)\\cdot\\P(y\\mid \\theta) \\cdot \\P(\\theta) $.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False intuition but good mnemonic\n",
    "We pretend to have a decent estimate $\\tilde\\theta$ of true $\\theta_\\mathrm{true}$ and we want to ameliorate it.\n",
    "\n",
    "In the E-step we calculate $\\tilde P(y) := \\P(y\\mid \\tilde\\theta, z)$ which we hope approximates $\\P(y\\mid \\theta_\\mathrm{true}, z)$. \n",
    "\n",
    "In the M-step we use the estimate $\\tilde P$ to get better $\\tilde \\theta$. If we knew the value $y$ of $Y$ exactly, we could find our $\\theta$ by maximizing $\\log\\P(z, y, \\theta) $. We do not know $y$ exactly, only its approximate distribution $\\tilde P(y)$. Thus it is natural to look for $\\theta$ by maximizing $\\E_{\\tilde P(y)}\\log\\P(z, y, \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typical case\n",
    "* $Z = (Z_i;\\, i \\in \\{1,\\ldots n\\})$; $Y = (Y_i;\\, i \\in \\{1,\\ldots n\\})$\n",
    "* $\\P(Y|\\theta) = \\prod_i \\P(Y_i|\\theta)$ (i.e. $Y_i$ are independent conditionaly on $\\theta$)\n",
    "* $\\P(Z|Y, \\theta) = \\prod_i \\P(Z_i| Y_i, \\theta)$\n",
    "\n",
    "Then the above EM-algorithm can be formulated as\n",
    "* **E-step:** for fixed $\\theta$ and each $i$ calculate \"posterior density of $Y_i$ given $z_i$, $\\theta$\":\n",
    "$$ P_i(y) := \\P(Y_i = y\\mid \\theta, z_i) \\propto \\P(Y_i = y\\mid \\theta) \\cdot \\P(z_i\\mid \\theta, Y_i = y)$$\n",
    "* **M-step:** Now fix all $P_i$ and maximize\n",
    "$$\\theta = \\argmax{\\theta}\\; \\bigg\\{\\log\\P(\\theta) + \\sum_i\\E_{P_i(y)}\\Big[\\log\\P(z_i, Y_i = y| \\theta) \\Big] \\bigg\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pista's implementation\n",
    "* **E-step:** for fixed $\\tilde\\theta$ use MCMC (Metropolis-Hastings) to draw samples $\\{y^{(j)}\\mid j \\in \\{1,\\ldots, m \\}\\}$ from the unnormalized density\n",
    "$$y\\mapsto  \\P(y\\mid \\tilde\\theta) \\cdot \\P(z\\mid \\tilde\\theta, y)$$\n",
    "* **M-step:** Now fix the samples $\\{y^{(j)}\\}$ and maximize\n",
    "$$\\tilde\\theta = \\argmax{\\theta}\\; \\sum_j\\log\\Big(\\P(z\\mid \\theta, y^{(j)}) \\cdot\\P(y^{(j)}\\mid \\theta) \\cdot \\P(\\theta)\\Big) $$\n",
    "\n",
    "##### Implementation details\n",
    "In the usual case $(Z, Y)$ is composed of many indepednent (conditionaly on $\\theta$) components $(Z_i, Y_i); \\ i \\in \\{1, \\ldots, n\\}$. Thus the the MCMC stepping can be done simultaneously and independently with all components.\n",
    "\n",
    "Another point is that drawing $m$ samples\n",
    "$$y^{(j)} = \\big[y^{(j)}_1,\\ldots, y^{(j)}_n\\big] \\quad j\\in \\{1,\\ldots, m\\}$$\n",
    "in the E-step is equivalent to pretending that we have $m$-times more observations (just copy them $m$-times):\n",
    "$$Z_{1}, \\ldots Z_{n},\\; Z_{1}, \\ldots Z_{n},\\; \\ldots Z_{1}, \\ldots Z_{n},\\;$$\n",
    "and drawing only one sample $y^{\\mathrm{sample}}$. Thus the algorithm simplifies as\n",
    "* **E-step:** with fixed $\\theta$ make several MCMC updates of all $y$ with the unnormalized densities\n",
    "$$y \\mapsto  \\P(y\\mid \\theta) \\cdot \\P(z\\mid \\theta, y)$$\n",
    "(Each such update is a batch of independent MCMC updates of all $y_i$ with the unnormalized densities $y_i\\mapsto  \\P(y_i\\mid \\theta) \\cdot \\P(z_i\\mid \\theta, y_i)$).\n",
    "* **M-step:** with fixed $y$ make several gradient-descent steps in $\\theta$ as if trying to maximize the function of $\\theta$\n",
    "$$\\theta \\mapsto \\log\\big(\\P(\\theta) \\cdot\\P(y\\mid \\theta) \\cdot \\P(z\\mid \\theta, y)\\big)$$\n",
    "\n",
    "Actually, when the number $n$ of observations is large one can take $m = 1$ i.e. not copy the observations.\n",
    "\n",
    "**Remark:** (Similarity to joint $(y,\\theta)$-posterior maximization) The maximization of the joint posterior\n",
    "$$ \\P(y, \\theta\\mid z) \\propto \\P(y, \\theta, z ) = \\P(\\theta) \\cdot\\P(y\\mid \\theta) \\cdot \\P(z\\mid \\theta, y)$$\n",
    "can be also done by repeating two maximization steps\n",
    "* **$M_y$-step:** with fixed $\\theta$ maximize w.r.t. $y$\n",
    "$$y \\mapsto  \\P(y, \\theta, z ) \\propto \\P(y\\mid \\theta) \\cdot \\P(z\\mid \\theta, y)$$\n",
    "(Each such update is a batch of independent maximizations for all $y_i$ of the unnormalized densities $y_i\\mapsto  \\P(y_i\\mid \\theta) \\cdot \\P(z_i\\mid \\theta, y_i)$).\n",
    "* **$M_\\theta$-step:** with fixed $y$ maximize w.r.t. $\\theta$ \n",
    "$$\\theta \\mapsto  \\P(y, \\theta, z ) = \\P(\\theta) \\cdot\\P(y\\mid \\theta) \\cdot \\P(z\\mid \\theta, y)$$\n",
    "\n",
    "This analogy becomes clearer in the context of variational inference. Note that the standard simplest algorithms for clusterization and linear dimensionality reduction are special cases of this \"MM\" algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasonability of the EM-algorithm\n",
    "\n",
    "### Prerequisities\n",
    "$\\newcommand{\\KL}[2]{D_{KL}\\left({#1}\\middle\\| {#2}\\right)}$\n",
    "**Definitions:**\n",
    "Let $P$, $Q$ be probability densities on some set (wrt. some uderlying measure).\n",
    "* Entropy: $$ H(P) = \\sum_y P(y) \\ \\log\\frac{1}{P(y)}$$\n",
    "* Kullback-Leibler divergence $$\\KL{P}{Q} = \\sum_y P(y)\\  \\log\\frac{P(y)}{Q(y)}$$\n",
    "\n",
    "**Lemma:** (Gibbs inequality)\n",
    "If $P$, $Q$ are as above then \n",
    "$$\\KL{P}{Q} \\geq 0 \\quad\\left(\\text{ equivalently }\\sum_y P(y)\\  \\log\\frac{1}{P(y)} \\leq \\sum_y P(y)\\  \\log\\frac{1}{Q(y)}\\right)$$\n",
    "with equality only if $P(y) = Q(y)$ almost everywhere.\n",
    "\n",
    "**Lemma:** (Jensen's inequality)\n",
    "Let $X:\\Omega \\to \\R$ be a random variable and let $f:\\R\\to\\R$ be a convex function. Then \n",
    "$$f(\\E X) \\leq \\E\\big[ f(X)\\big].$$\n",
    "If $f$ is strictly convex then the equality holds only in the case when $X$ is a constant (almost everywhere).\n",
    "\n",
    "**Proof of Gibbs**: Use Jensen for strictly convex $f(u) = -\\log(u)$ and a random variable having values $\\frac{Q(y)}{P(y)}$ with probas $P(y)$ (i.e. $\\Omega$ is the set of all possible values of $y$).\n",
    "$$\\KL{P}{Q} = \\sum_y P(y)\\cdot  f\\left(\\frac{Q(y)}{P(y)}\\right) \\geq  f\\left(\\sum_y P(y)\\cdot  \\frac{Q(y)}{P(y)}\\right) = f(1) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function $F$ that increases in each step\n",
    "Define a function $F$ of two variables $P, \\theta$ there $P$ is a probability measure on $y$ and $\\theta$ is well, a value of model variables $\\theta$.\n",
    "$$\n",
    "\\begin{align}\n",
    "F(P, \\theta) & := \\E_P\\Big[\\log \\P(y, z, \\theta)\\Big] + H(P) =\\tag{F1}\\\\ \n",
    "& = \\E_P\\Big[\\log \\P(y, z\\mid \\theta)\\Big] + H(P) + \\log\\P(\\theta) = \\\\\n",
    "& = \\sum_y P(y)\\log\\frac{\\P(y,z\\mid \\theta)}{P(y)} + \\log\\P(\\theta) = \\\\\n",
    "&= -D_{KL}\\big[P(y)\\ \\big\\|\\  \\P(y\\mid z, \\theta)\\big] + \\log\\big[\\P(z\\mid\\theta) \\P(\\theta)\\big] \\tag{F2}\n",
    "\\end{align}\n",
    "$$\n",
    "Here the 1st and the last line are important.\n",
    "Using $F$ we van reformulate the EM-algorithm:\n",
    "* **E-step:** For fixed $\\tilde\\theta$:\n",
    "$$\\tilde P := \\argmax{P}\\; F(P, \\tilde\\theta)$$\n",
    "* **M-step:** For fixed $\\tilde P$:\n",
    "$$\\tilde\\theta = \\argmax{\\theta}\\; F(\\tilde P, \\theta)$$\n",
    "\n",
    "Here the equivalence of E-steps is ensured by (F2) and the equivalence of M-steps by (F1).\n",
    "\n",
    "In the view of this formulation there is a reasonable hope that the EM algorithm will converge to a maximum of $F$. (Of course it can converge to a local maximum or a saddle point. I don't even see any reason why $P,\\theta$ should converge at all (probably usualy some compactness arguments can be used). Of course the values of $F(P,\\theta)$ converge.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The maximum of $F$ is what we want\n",
    "Denote $\\hat P, \\hat \\theta$ the maximum of $F$. From the above, we see that $\\hat P(y) = \\P(y\\mid z,\\theta)$ and thus \n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\theta &= \\argmax{\\theta}\\Big\\{\\E_{\\P(y\\mid z,\\theta)} \\big[\\log\\P(y, z, \\theta)\\big]  + H\\big[\\P(y\\mid z, \\theta)\\big] \\Big\\}=\\\\\n",
    "&=\\argmax{\\theta}\\sum_y \\P(y\\mid z, \\theta) \\ \\log\\frac{\\P(y, z, \\theta)}{\\P(y\\mid z,\\theta)} = \\quad\\bigg(\\text{now use } \\frac{\\P(y, z, \\theta)}{\\P(y\\mid z,\\theta)} = \\P(z, \\theta)\\bigg) \\\\\n",
    "&= \\argmax{\\theta}\\Big\\{ \\log\\P(z,\\theta)\\cdot\\sum_y\\P(y\\mid z,\\theta)\\Big\\} = \\\\\n",
    "&= \\argmax{\\theta}\\; \\log\\P(z, \\theta) = \\argmax{\\theta}\\; \\log\\P(\\theta\\mid z) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:18:24.031059Z",
     "start_time": "2018-08-11T09:18:24.021058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2\n",
    "x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:19:11.709728Z",
     "start_time": "2018-08-11T09:19:11.555725Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:19:52.773444Z",
     "start_time": "2018-08-11T09:19:52.639442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2195792d3c8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-2, 3, 200)\n",
    "pl.plot(x, x**3 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational methods\n",
    "Here we review some basics about variational inference in order to see that the mysterious function $F$ from EM-algorithm appears naturally in this context.\n",
    "\n",
    "#### General problem\n",
    "Consider\n",
    "* $U(x)$ - un-normalized density on $x$\n",
    "* $Q(x, q)$ - family of normalized proba-densities on $x$ parametrized by $q$\n",
    "\n",
    "Denote\n",
    "* $Z:= \\sum_x U(x)$ so that $\\frac{U(x)}{Z}$ is a normalized density on $x$.\n",
    "\n",
    "Problems:\n",
    "* find $q$ s.t. $Q(x, q)$ optimaly (in some sense) approximates $\\frac{U(x)}{Z}$\n",
    "* estimate $Z$\n",
    "\n",
    "#### Minimization of Kullback-Leibler divergence\n",
    "One possible distance between $Q(x, q)$ and $\\frac{U(x)}{Z}$ is the KL-divergence. Thus we want to minimize\n",
    "$$\\KL{Q(x, q)}{\\frac U Z}.$$\n",
    "The slight hindrance is that we don't know $Z$. But we can introduce the \"divergence without normalizing factor\" (and with a minus sign):\n",
    "$$ \n",
    "\\begin{align}\n",
    "F(q) &= \\sum_x Q(x, q) \\log\\frac{U(x)}{Q(x, q)} = \\\\\n",
    "&= -\\KL{Q(x, q)}{\\frac U Z} + \\log Z\n",
    "\\end{align}\n",
    "$$\n",
    "So the minimization of $D_{K,L}$ is equivalent to maximization of $F$.\n",
    "\n",
    "Actually, the maximal value of $F$ is usualy a reasonable approximation of $\\log Z$.$\\newcommand{qm}{q_{max}}$ Denote $\\qm$ the value of $q$ maximizing $F(q)$. If the family $Q$ is rich enough we can expect $\\KL{Q(x, \\qm)}{\\frac U Z}$ to be small (and positive by Gibbs). So from \n",
    "$$F(\\qm) = -\\KL{Q(x, \\qm)}{\\frac U Z} + \\log Z$$\n",
    "we can expect $F(\\qm)$ to be a reasonable lower estimate on $\\log Z$.\n",
    "\n",
    "There are two use cases and corresponding terminologies: Bayesian statistics and statistical physics.\n",
    "#### Bayesian incarnation\n",
    "In a Bayesian statistical model we have unknown parameters $\\theta$, observations $y$ and we are given the following functions\n",
    "* $\\theta\\mapsto \\P(\\theta)$ - prior on $\\theta$\n",
    "* $y, \\theta \\mapsto \\P(y\\mid \\theta)$ - likelihood function.\n",
    "\n",
    "In this context\n",
    "* $U(\\theta) = \\P(\\theta) \\P(y\\mid \\theta)$ is the unnormalized Bayesian posterior\n",
    "* $Z = \\sum_\\theta U(\\theta) = \\P(y)$ is the **evidence** of the current model. Actualy, sometimes $\\log Z$ is called evidence. \n",
    "* $F(q)$ is called the **Evidence lower bound** since by Gibbs $F(q) \n",
    "\\geq \\log Z$\n",
    "\n",
    "#### Statistical physics (please skip)\n",
    "Consider a physical system with given energy function $$x\\mapsto E(x) \\quad \\text{(here $x$ is a microstate)}$$ and inverse temperature $\\beta$. (The relation to the ordinary temperature is $\\beta = \\frac{1}{k\\cdot T}$ where $T$ is the temperature and $k$ is the Boltzman constant. Thus $\\beta \\cdot E(x)$ has no units.)\n",
    "* $U(x) = \\exp\\left(-\\beta \\ E(x)\\right)$ is the unnormalized Boltzman distribution\n",
    "* $Z(\\beta) = \\sum_x \\exp\\left(-\\beta \\ E(x)\\right)$ is called the **partition function** or **statistical sum**\n",
    "* **Variational free energy** is also denoted by $F$ but has a different sign and is rescaled by $\\beta$:\n",
    "$$F_{\\mathrm{physics}}(q, \\beta) = -\\frac{1}{\\beta} F(q)=-\\frac{1}{\\beta}\\sum_x Q(x, q) \\log\\frac{U(x)}{Q(x, q)}$$\n",
    "Thus $F_{\\mathrm{physics}}$ has the same units as energy and we want to minimize it (as usualy happens with energies).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximal posterior as a variational method\n",
    "We return to the simple Bayesian model: $\\theta$ are the unknown model parameters and $Y$ is the observation. The unnormalized posterior is\n",
    "$$U(\\theta) = \\P(\\theta) \\P(y\\mid \\theta)$$\n",
    "\n",
    "For simplicity assume that the parameter $\\theta$ is discrete.\n",
    "As our variational family $Q$ we choose the delta-distributions on the space of model parameters and denote the variational parameter by $\\tilde\\theta$:\n",
    "$$ Q_{\\tilde\\theta} (\\theta) = \\delta_{\\tilde\\theta}(\\theta)$$\n",
    "We want to minimize the $D_{KL}\\big[\\delta_{\\tilde\\theta}(\\theta),\\ \\P(\\theta\\mid z)\\big]$ what amounts to maximizing\n",
    "$$\n",
    "\\begin{align}\n",
    "F(\\tilde\\theta) &= \\sum_\\theta \\delta_{\\tilde\\theta}(\\theta)\\log\\frac{U(\\theta)}{\\delta_{\\tilde\\theta}(\\theta)} =\\\\\n",
    "&=\\log U(\\tilde\\theta) - \\log\\delta_{\\tilde\\theta}(\\tilde\\theta)=\\log U(\\tilde\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Remark**: We used here that $\\delta_{\\tilde\\theta}(\\tilde\\theta) = 1$ and thus $\\log\\delta_{\\tilde\\theta}(\\tilde\\theta) = 0$ if $\\theta$ is discrete. If $\\theta$ is not discrete then strictly speaking $\\delta_{\\tilde\\theta}(\\tilde\\theta) = \\infty$. However, one can think of $\\delta_{\\tilde\\theta}$ as $\\delta_{\\tilde\\theta} \\sim \\frac{1}{d\\theta}\\chi_{[\\tilde\\theta, \\tilde\\theta + d\\theta]}$ (in the case when $\\theta\\in \\R$) so $\\log\\delta_{\\tilde\\theta}(\\tilde\\theta) \\sim \\log\\frac{1}{d\\theta}$ is just a large constant and one can disregard it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T15:43:13.965460Z",
     "start_time": "2018-04-09T15:43:13.958459Z"
    }
   },
   "outputs": [],
   "source": [
    "#Note however that the meaning of the interval $[\\tilde\\theta, \\tilde\\theta + d\\theta]$ depends on the choice of coordinates. This is to be expected however -- the maximal posterior itself depends on coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to EM-algorithm\n",
    "We return to the notation used in the discussion of EM-algorithm:\n",
    "* $\\theta$ - unknown model variables\n",
    "* $Y$ - unknown latent variables\n",
    "* $Z$ - observations.\n",
    "\n",
    "Bayes gives us the posterior on $\\theta, Y$:\n",
    "$$\\P(y, \\theta\\mid z) \\propto \\P(y, \\theta, z)$$\n",
    "Our variational family $Q$ will be parametrized by two parameters: \n",
    "* $\\tilde P(y)$ - a proba measure on $y$\n",
    "* $\\tilde\\theta$ - a value of the model params.\n",
    "\n",
    "The proba measure $Q_{(\\tilde P, \\tilde \\theta)}$ will be just the cartesian probuct of the two measures:\n",
    "$$Q_{(\\tilde P, \\tilde \\theta)}(y, \\theta) = \\tilde P(y) \\cdot \\delta_{\\tilde\\theta}(\\theta).$$\n",
    "Calculate the ELBO:\n",
    "$$\n",
    "\\begin{align}\n",
    "F(\\tilde P, \\tilde\\theta) &= \\sum_{y, \\theta} Q_{(\\tilde P, \\tilde\\theta)}(y, \\theta)\\cdot \\log\\frac{\\P(z, y, \\theta)}{Q_{(\\tilde P, \\tilde\\theta)}(y, \\theta)} =\\\\\n",
    "&= \\sum_y \\tilde P(y) \\cdot \\log\\frac{\\P(z, y, \\tilde\\theta)}{\\tilde P (y)}- \\log\\delta_{\\tilde\\theta}(\\tilde\\theta) = \\quad\\big(\\text{if $\\theta$ is discrete  then $\\log\\delta_{\\tilde\\theta}(\\tilde\\theta) = 0$}\\big) \\\\\n",
    "&= \\E_{\\tilde P} \\big[\\log\\P(z,y, \\tilde\\theta)\\big] + H(\\tilde P)\n",
    "\\end{align}\n",
    "$$\n",
    "So we see that we get the same function $F$ as in the \"proof\" of the EM-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "191px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
