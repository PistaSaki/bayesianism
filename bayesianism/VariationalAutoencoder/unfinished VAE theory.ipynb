{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{P}{\\mathbb{P}}$\n",
    "$\\newcommand{R}{\\mathbb{R}}$\n",
    "$\\newcommand{Z}{\\mathbb{Z}}$\n",
    "$\\newcommand{E}{\\mathbb{E}}$\n",
    "$\\newcommand{x}{\\mathbf{x}}$\n",
    "$\\newcommand{y}{\\mathbf{y}}$\n",
    "$\\newcommand{z}{\\mathbf{z}}$\n",
    "$\\newcommand{Norm}{\\mathrm{Norm}}$\n",
    "$\\newcommand{\\KL}[2]{D_{KL}\\left[{#1}\\middle\\| {#2}\\right]}$\n",
    "$\\newcommand{ELBO}{\\mathcal{L}_{\\theta, \\varphi}(x)}$\n",
    "\n",
    "\n",
    "\n",
    "# Variational Autoencoders\n",
    "The model and its notation follows Kingma's disertation. (Thus the notation is a bit different from `EM-Algorithm-theory.ipynb` and arguably even slopier.) The presentation is heavily based on [Louis Tiao's blog](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/).\n",
    "\n",
    "We have a Bayesian model containing random variables\n",
    "The typical statistical problem comprises random variables\n",
    "* $\\theta$ - the unknown model variables we are interested in.\n",
    "* $Z$ - the unknown latent variables we don't care about. \n",
    "Usualy the dimension increases with number of observations i.e. we have $Z_i$; $i \\in \\{1, \\ldots, n\\}$.\n",
    "* $X$ - observations. Usualy composed of independent parts $X_i$.\n",
    "\n",
    "Moreover we (pretend to) know the following probas:\n",
    "* $p(\\theta) := \\P(\\theta)$ - prior on model parameters\n",
    "* $p(z):=\\P(Z)$ - prior on latent variables. Usualy decomposes as $\\P(Y) = \\prod_i\\P(Y_i)$. \n",
    "* $p_\\theta(x\\mid z) := \\P(X\\mid Z,\\ \\theta)$ - \"likelihood\". Usualy $\\P(X\\mid Z,\\ \\theta) = \\prod_i \\P(X_i\\mid Z_i,\\ \\theta)$\n",
    "\n",
    "**Remark**: This is a small simplification in comparison `EM-Algorithm-theory.ipynb` -- the prior on latent variables is fixed and does not depend on $\\theta$. I think it is not significant and we don't have to assume that. However in the usual implementation of VAEs, this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational inference for VAEs\n",
    "#### Lower approximation of $\\log \\P(x\\mid \\theta)$\n",
    "Fix a variational family $q_\\varphi(z\\mid x)$ of distributions on latent variables $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta(x) &= \\E_{q_\\varphi(z\\mid x)}\\left[\\log\\frac{p_\\theta(x, z)}{q_\\varphi(z\\mid x)}\\right] + \\E_{q_\\varphi(z\\mid x)}\\left[ \\log\\frac{q_\\varphi(z\\mid x)}{p_\\theta(z\\mid x)} \\right] =\\\\\n",
    "&= \\ELBO + \\KL{q_\\varphi(z\\mid x)\\,}{\\,p_\\theta(z\\mid x)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\E_{q_\\varphi(z\\mid x)}\\left[\\log\\frac{p_\\theta(x, z)}{q_\\varphi(z\\mid x)}\\right] =: \\ELBO $ is called \"evidence lower bound\" or ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective for VAE\n",
    "In the original problem, we might want to find the MAP estimator of $\\theta$ i.e. to maximize $ \\P[\\theta \\mid x] \\propto  p(\\theta)p_\\theta(x) $ wrt $\\theta$. Thus in the variational approximation we maximize $\\log p(\\theta) + \\ELBO$ wrt $\\theta, \\varphi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the optimization, people use the following expression for ELBO:\n",
    "$$\\ELBO = \\E_{q_\\varphi(z\\mid x)}\\left[\\log {p_\\theta(x\\mid z)}\\right] - \\KL{q_\\varphi(z\\mid x)\\,}{\\,p_\\theta(z)}$$\n",
    "Usualy both $q_\\varphi(z\\mid x)$ and $p_\\theta(z)$ are Gaussians, thus the second term can be evaluated explicitly. I think people prefer this instead of the definition because when estimating it via sampling, it has lower variance. But I don't know whether it is so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "$$q_\\phi(\\z\\mid\\x) = \\prod_i q_\\phi(\\z_i\\mid \\x_i)$$\n",
    "Where\n",
    "$$q_\\phi(\\z_i\\mid \\x_i) = \\Norm\\left[\\z_i\\mid \\mu_\\phi(\\x_i), \\mathrm{diag}\\left(\\sigma_\\phi^2(\\x_i)\\right) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "(x_train, y_train, x_test, y_test) = [u[..., None].astype(np.float32)/255 for u in (x_train, y_train, x_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), dtype('float32'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_encoder(img_shape, latent_dim):\n",
    "#     x = inp = kl.Input(img_shape)\n",
    "#     x = kl.Conv2D(4, (3, 3), activation=\"relu\")(x)\n",
    "#     x = kl.MaxPool2D()(x)\n",
    "#     x = kl.Conv2D(8, (3, 3), activation=\"relu\")(x)\n",
    "#     x = kl.MaxPool2D()(x)\n",
    "#     x = kl.Flatten()(x)\n",
    "    \n",
    "#     mu = kl.Dense(latent_dim, activation=\"relu\")(x)\n",
    "    \n",
    "#     log_var = kl.Dense(latent_dim, activation=\"relu\")(x)\n",
    "    \n",
    "#     return keras.Model(inputs=inp, outputs=[mu, log_var])\n",
    "\n",
    "def build_encoder(img_shape, latent_dim, intermediate_dim = 256):\n",
    "    \n",
    "    x = inp = kl.Input(shape=img_shape)\n",
    "    x = kl.Flatten()(x)\n",
    "    h = kl.Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "    z_mu = kl.Dense(latent_dim)(h)\n",
    "    z_log_var = kl.Dense(latent_dim)(h)\n",
    "    return keras.Model(inp, [z_mu, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder(\n",
    "    img_shape = x_train.shape[1:],\n",
    "    latent_dim = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 784)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          200960      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            514         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            514         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 201,988\n",
      "Trainable params: 201,988\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=205, shape=(5, 2), dtype=float32, numpy=\n",
       " array([[-0.10622245,  0.6848679 ],\n",
       "        [ 0.20929775, -0.33045852],\n",
       "        [ 0.23429798, -0.51155263],\n",
       "        [ 0.3063422 ,  0.04426567],\n",
       "        [ 0.3082237 , -0.15488243]], dtype=float32)>,\n",
       " <tf.Tensor: id=201, shape=(5, 2), dtype=float32, numpy=\n",
       " array([[-0.2463617 ,  0.25155574],\n",
       "        [-0.10468969,  0.45278692],\n",
       "        [-0.34343222, -0.01106612],\n",
       "        [ 0.08569366,  0.03645107],\n",
       "        [ 0.18872315,  0.6835544 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.regularizers as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_decoder(latent_dim, img_shape):\n",
    "#     x = inp = kl.Input([latent_dim])\n",
    "#     x = kl.Dense(64, kernel_regularizer=reg.l2(1e-5), activation=\"relu\")(x)\n",
    "#     x = kl.Dense(np.prod(img_shape), activation=\"sigmoid\")(x)\n",
    "#     x = kl.Reshape(img_shape)(x)\n",
    "    \n",
    "#     return keras.Model(inputs=inp, outputs=x)\n",
    "\n",
    "def build_decoder(img_shape, latent_dim, intermediate_dim = 256):\n",
    "\n",
    "    return keras.Sequential([\n",
    "        kl.Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "        kl.Dense(np.prod(img_shape), activation='sigmoid'),\n",
    "        kl.Reshape(img_shape)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = build_decoder(latent_dim=2, img_shape=x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=413, shape=(10, 28, 28, 1), dtype=float32, numpy=\n",
       "array([[[[0.51077366],\n",
       "         [0.5025334 ],\n",
       "         [0.50073105],\n",
       "         ...,\n",
       "         [0.50151366],\n",
       "         [0.5072017 ],\n",
       "         [0.50278527]],\n",
       "\n",
       "        [[0.4964475 ],\n",
       "         [0.51020914],\n",
       "         [0.5005751 ],\n",
       "         ...,\n",
       "         [0.5088207 ],\n",
       "         [0.50628984],\n",
       "         [0.5017302 ]],\n",
       "\n",
       "        [[0.497089  ],\n",
       "         [0.50346375],\n",
       "         [0.49220026],\n",
       "         ...,\n",
       "         [0.49703255],\n",
       "         [0.5031325 ],\n",
       "         [0.49116737]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49116457],\n",
       "         [0.5065324 ],\n",
       "         [0.48972404],\n",
       "         ...,\n",
       "         [0.51555115],\n",
       "         [0.5019425 ],\n",
       "         [0.49947608]],\n",
       "\n",
       "        [[0.5033738 ],\n",
       "         [0.49776432],\n",
       "         [0.5085329 ],\n",
       "         ...,\n",
       "         [0.5076323 ],\n",
       "         [0.49837822],\n",
       "         [0.49694142]],\n",
       "\n",
       "        [[0.5066629 ],\n",
       "         [0.50183606],\n",
       "         [0.4917495 ],\n",
       "         ...,\n",
       "         [0.5009219 ],\n",
       "         [0.5104325 ],\n",
       "         [0.5005448 ]]],\n",
       "\n",
       "\n",
       "       [[[0.51675904],\n",
       "         [0.50506717],\n",
       "         [0.50040346],\n",
       "         ...,\n",
       "         [0.5035565 ],\n",
       "         [0.5105423 ],\n",
       "         [0.5048973 ]],\n",
       "\n",
       "        [[0.49489796],\n",
       "         [0.5154269 ],\n",
       "         [0.5006022 ],\n",
       "         ...,\n",
       "         [0.514008  ],\n",
       "         [0.5101746 ],\n",
       "         [0.5033768 ]],\n",
       "\n",
       "        [[0.4941666 ],\n",
       "         [0.50253445],\n",
       "         [0.4881166 ],\n",
       "         ...,\n",
       "         [0.49570468],\n",
       "         [0.5026042 ],\n",
       "         [0.48716444]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.48707157],\n",
       "         [0.51056176],\n",
       "         [0.4846004 ],\n",
       "         ...,\n",
       "         [0.52565134],\n",
       "         [0.502558  ],\n",
       "         [0.499989  ]],\n",
       "\n",
       "        [[0.50382847],\n",
       "         [0.49557474],\n",
       "         [0.51183057],\n",
       "         ...,\n",
       "         [0.511046  ],\n",
       "         [0.49812537],\n",
       "         [0.49435005]],\n",
       "\n",
       "        [[0.5110303 ],\n",
       "         [0.50303924],\n",
       "         [0.48786336],\n",
       "         ...,\n",
       "         [0.5013308 ],\n",
       "         [0.51598084],\n",
       "         [0.49989548]]],\n",
       "\n",
       "\n",
       "       [[[0.5049618 ],\n",
       "         [0.5061706 ],\n",
       "         [0.49652797],\n",
       "         ...,\n",
       "         [0.5074987 ],\n",
       "         [0.5020439 ],\n",
       "         [0.499648  ]],\n",
       "\n",
       "        [[0.5016308 ],\n",
       "         [0.5006756 ],\n",
       "         [0.50190145],\n",
       "         ...,\n",
       "         [0.50568837],\n",
       "         [0.50629073],\n",
       "         [0.5017344 ]],\n",
       "\n",
       "        [[0.4945192 ],\n",
       "         [0.49239746],\n",
       "         [0.49657923],\n",
       "         ...,\n",
       "         [0.5013243 ],\n",
       "         [0.49631706],\n",
       "         [0.49967235]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49925172],\n",
       "         [0.5042983 ],\n",
       "         [0.49711344],\n",
       "         ...,\n",
       "         [0.50914526],\n",
       "         [0.49964383],\n",
       "         [0.5028564 ]],\n",
       "\n",
       "        [[0.49733114],\n",
       "         [0.4977181 ],\n",
       "         [0.5010287 ],\n",
       "         ...,\n",
       "         [0.5041953 ],\n",
       "         [0.50069636],\n",
       "         [0.49610093]],\n",
       "\n",
       "        [[0.5039626 ],\n",
       "         [0.500937  ],\n",
       "         [0.5006568 ],\n",
       "         ...,\n",
       "         [0.49761623],\n",
       "         [0.504068  ],\n",
       "         [0.49499577]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.50270045],\n",
       "         [0.50129837],\n",
       "         [0.5046436 ],\n",
       "         ...,\n",
       "         [0.49955222],\n",
       "         [0.5053679 ],\n",
       "         [0.49996457]],\n",
       "\n",
       "        [[0.49637717],\n",
       "         [0.5047701 ],\n",
       "         [0.50139046],\n",
       "         ...,\n",
       "         [0.5022831 ],\n",
       "         [0.5043617 ],\n",
       "         [0.50098616]],\n",
       "\n",
       "        [[0.5031048 ],\n",
       "         [0.50669646],\n",
       "         [0.49594802],\n",
       "         ...,\n",
       "         [0.49969694],\n",
       "         [0.50793034],\n",
       "         [0.49472207]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49586046],\n",
       "         [0.4980133 ],\n",
       "         [0.49340734],\n",
       "         ...,\n",
       "         [0.5007601 ],\n",
       "         [0.50325674],\n",
       "         [0.49492657]],\n",
       "\n",
       "        [[0.5051174 ],\n",
       "         [0.50158733],\n",
       "         [0.51186216],\n",
       "         ...,\n",
       "         [0.508791  ],\n",
       "         [0.49532995],\n",
       "         [0.500393  ]],\n",
       "\n",
       "        [[0.49871588],\n",
       "         [0.49720478],\n",
       "         [0.49720562],\n",
       "         ...,\n",
       "         [0.4995508 ],\n",
       "         [0.50438976],\n",
       "         [0.5023075 ]]],\n",
       "\n",
       "\n",
       "       [[[0.51005405],\n",
       "         [0.51247066],\n",
       "         [0.49256223],\n",
       "         ...,\n",
       "         [0.5158848 ],\n",
       "         [0.50306314],\n",
       "         [0.49882966]],\n",
       "\n",
       "        [[0.5046022 ],\n",
       "         [0.4996653 ],\n",
       "         [0.50393975],\n",
       "         ...,\n",
       "         [0.5104079 ],\n",
       "         [0.51299906],\n",
       "         [0.5034193 ]],\n",
       "\n",
       "        [[0.48829314],\n",
       "         [0.48375562],\n",
       "         [0.49420592],\n",
       "         ...,\n",
       "         [0.50324744],\n",
       "         [0.49233794],\n",
       "         [0.5009034 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49924925],\n",
       "         [0.5086639 ],\n",
       "         [0.49495253],\n",
       "         ...,\n",
       "         [0.51657945],\n",
       "         [0.4995452 ],\n",
       "         [0.506351  ]],\n",
       "\n",
       "        [[0.49421418],\n",
       "         [0.49578172],\n",
       "         [0.5020751 ],\n",
       "         ...,\n",
       "         [0.5089259 ],\n",
       "         [0.50186455],\n",
       "         [0.49244046]],\n",
       "\n",
       "        [[0.5074848 ],\n",
       "         [0.5006958 ],\n",
       "         [0.50218654],\n",
       "         ...,\n",
       "         [0.49446422],\n",
       "         [0.5068912 ],\n",
       "         [0.4894075 ]]],\n",
       "\n",
       "\n",
       "       [[[0.51625484],\n",
       "         [0.5136609 ],\n",
       "         [0.49350718],\n",
       "         ...,\n",
       "         [0.5132932 ],\n",
       "         [0.5091232 ],\n",
       "         [0.50304055]],\n",
       "\n",
       "        [[0.49822354],\n",
       "         [0.51126367],\n",
       "         [0.5024628 ],\n",
       "         ...,\n",
       "         [0.5168897 ],\n",
       "         [0.51529765],\n",
       "         [0.5056521 ]],\n",
       "\n",
       "        [[0.48836634],\n",
       "         [0.4870407 ],\n",
       "         [0.4871632 ],\n",
       "         ...,\n",
       "         [0.49889567],\n",
       "         [0.49335796],\n",
       "         [0.49212623]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49239632],\n",
       "         [0.5123392 ],\n",
       "         [0.48705688],\n",
       "         ...,\n",
       "         [0.5309608 ],\n",
       "         [0.5009711 ],\n",
       "         [0.50559527]],\n",
       "\n",
       "        [[0.49747276],\n",
       "         [0.4928193 ],\n",
       "         [0.5055328 ],\n",
       "         ...,\n",
       "         [0.5099379 ],\n",
       "         [0.49962878],\n",
       "         [0.48876962]],\n",
       "\n",
       "        [[0.5128335 ],\n",
       "         [0.50462687],\n",
       "         [0.49458492],\n",
       "         ...,\n",
       "         [0.49668798],\n",
       "         [0.51549697],\n",
       "         [0.49026263]]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = decoder(tfd.Normal(0, 1).sample([10, 2]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        mu, log_var = self.encoder(x)\n",
    "        kl_batch = - 1/2 *  K.sum(\n",
    "            1 + log_var - mu**2 - K.exp(log_var), axis=-1)\n",
    "        self.add_loss(K.mean(kl_batch))\n",
    "        \n",
    "        sigma = tf.exp(log_var / 2)\n",
    "        eps = tfd.Normal(0, 1).sample(mu.shape)\n",
    "        return self.decoder(mu + sigma * eps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=492, shape=(60000, 28, 28, 1), dtype=float32, numpy=\n",
       "array([[[[0.5205799 ],\n",
       "         [0.5022304 ],\n",
       "         [0.5067776 ],\n",
       "         ...,\n",
       "         [0.49773127],\n",
       "         [0.51655924],\n",
       "         [0.50339097]],\n",
       "\n",
       "        [[0.49211887],\n",
       "         [0.521359  ],\n",
       "         [0.50347424],\n",
       "         ...,\n",
       "         [0.51382416],\n",
       "         [0.5125198 ],\n",
       "         [0.5007136 ]],\n",
       "\n",
       "        [[0.50093603],\n",
       "         [0.5177573 ],\n",
       "         [0.48560604],\n",
       "         ...,\n",
       "         [0.49324083],\n",
       "         [0.5182523 ],\n",
       "         [0.47831127]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.4818416 ],\n",
       "         [0.5084122 ],\n",
       "         [0.47569558],\n",
       "         ...,\n",
       "         [0.5245856 ],\n",
       "         [0.50606155],\n",
       "         [0.49370295]],\n",
       "\n",
       "        [[0.5101913 ],\n",
       "         [0.50003535],\n",
       "         [0.5249921 ],\n",
       "         ...,\n",
       "         [0.5206972 ],\n",
       "         [0.490722  ],\n",
       "         [0.49749917]],\n",
       "\n",
       "        [[0.51051384],\n",
       "         [0.50169283],\n",
       "         [0.481861  ],\n",
       "         ...,\n",
       "         [0.5021418 ],\n",
       "         [0.52155966],\n",
       "         [0.50620013]]],\n",
       "\n",
       "\n",
       "       [[[0.5217013 ],\n",
       "         [0.5410317 ],\n",
       "         [0.48538178],\n",
       "         ...,\n",
       "         [0.5067782 ],\n",
       "         [0.50635254],\n",
       "         [0.50050604]],\n",
       "\n",
       "        [[0.5148632 ],\n",
       "         [0.49357802],\n",
       "         [0.5127917 ],\n",
       "         ...,\n",
       "         [0.47954288],\n",
       "         [0.55503416],\n",
       "         [0.51381916]],\n",
       "\n",
       "        [[0.4786212 ],\n",
       "         [0.4888072 ],\n",
       "         [0.47368947],\n",
       "         ...,\n",
       "         [0.4966716 ],\n",
       "         [0.49260136],\n",
       "         [0.5198647 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.47798663],\n",
       "         [0.50066733],\n",
       "         [0.49858144],\n",
       "         ...,\n",
       "         [0.47556648],\n",
       "         [0.4714284 ],\n",
       "         [0.5034284 ]],\n",
       "\n",
       "        [[0.49391615],\n",
       "         [0.49174127],\n",
       "         [0.5178485 ],\n",
       "         ...,\n",
       "         [0.5090639 ],\n",
       "         [0.5163041 ],\n",
       "         [0.50324893]],\n",
       "\n",
       "        [[0.4753401 ],\n",
       "         [0.48753121],\n",
       "         [0.49274397],\n",
       "         ...,\n",
       "         [0.49064603],\n",
       "         [0.5130577 ],\n",
       "         [0.48435223]]],\n",
       "\n",
       "\n",
       "       [[[0.50772154],\n",
       "         [0.5160227 ],\n",
       "         [0.49452677],\n",
       "         ...,\n",
       "         [0.4984479 ],\n",
       "         [0.5040386 ],\n",
       "         [0.5003151 ]],\n",
       "\n",
       "        [[0.5037781 ],\n",
       "         [0.49695167],\n",
       "         [0.50618553],\n",
       "         ...,\n",
       "         [0.48828134],\n",
       "         [0.5209288 ],\n",
       "         [0.5064107 ]],\n",
       "\n",
       "        [[0.4913645 ],\n",
       "         [0.4986707 ],\n",
       "         [0.49004576],\n",
       "         ...,\n",
       "         [0.4971475 ],\n",
       "         [0.49845436],\n",
       "         [0.5089544 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.48885882],\n",
       "         [0.5001779 ],\n",
       "         [0.4997428 ],\n",
       "         ...,\n",
       "         [0.4864909 ],\n",
       "         [0.48809248],\n",
       "         [0.49878752]],\n",
       "\n",
       "        [[0.49874958],\n",
       "         [0.49471515],\n",
       "         [0.5089336 ],\n",
       "         ...,\n",
       "         [0.50038546],\n",
       "         [0.50800383],\n",
       "         [0.5026116 ]],\n",
       "\n",
       "        [[0.4885234 ],\n",
       "         [0.49410972],\n",
       "         [0.49477935],\n",
       "         ...,\n",
       "         [0.49580655],\n",
       "         [0.50558794],\n",
       "         [0.4949461 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.5218162 ],\n",
       "         [0.5412139 ],\n",
       "         [0.48529807],\n",
       "         ...,\n",
       "         [0.5068723 ],\n",
       "         [0.5063642 ],\n",
       "         [0.50050473]],\n",
       "\n",
       "        [[0.5149762 ],\n",
       "         [0.4935614 ],\n",
       "         [0.5128441 ],\n",
       "         ...,\n",
       "         [0.4795112 ],\n",
       "         [0.5552649 ],\n",
       "         [0.5138755 ]],\n",
       "\n",
       "        [[0.47852877],\n",
       "         [0.4886828 ],\n",
       "         [0.47357568],\n",
       "         ...,\n",
       "         [0.49670205],\n",
       "         [0.4925346 ],\n",
       "         [0.5199367 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.47793913],\n",
       "         [0.50068665],\n",
       "         [0.49856967],\n",
       "         ...,\n",
       "         [0.47552675],\n",
       "         [0.47131744],\n",
       "         [0.5034817 ]],\n",
       "\n",
       "        [[0.49386895],\n",
       "         [0.49173975],\n",
       "         [0.51789576],\n",
       "         ...,\n",
       "         [0.50915605],\n",
       "         [0.5163381 ],\n",
       "         [0.5032174 ]],\n",
       "\n",
       "        [[0.4752905 ],\n",
       "         [0.48749024],\n",
       "         [0.4927539 ],\n",
       "         ...,\n",
       "         [0.490615  ],\n",
       "         [0.5131223 ],\n",
       "         [0.48426685]]],\n",
       "\n",
       "\n",
       "       [[[0.5045097 ],\n",
       "         [0.50509906],\n",
       "         [0.5157186 ],\n",
       "         ...,\n",
       "         [0.49901605],\n",
       "         [0.51616126],\n",
       "         [0.4993698 ]],\n",
       "\n",
       "        [[0.49083775],\n",
       "         [0.50996125],\n",
       "         [0.5063123 ],\n",
       "         ...,\n",
       "         [0.5047477 ],\n",
       "         [0.5087702 ],\n",
       "         [0.50677687]],\n",
       "\n",
       "        [[0.51066625],\n",
       "         [0.51979476],\n",
       "         [0.49012685],\n",
       "         ...,\n",
       "         [0.503803  ],\n",
       "         [0.5203195 ],\n",
       "         [0.4877515 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49099392],\n",
       "         [0.48980767],\n",
       "         [0.4880758 ],\n",
       "         ...,\n",
       "         [0.49179515],\n",
       "         [0.50928366],\n",
       "         [0.48079342]],\n",
       "\n",
       "        [[0.51954323],\n",
       "         [0.50872225],\n",
       "         [0.5382197 ],\n",
       "         ...,\n",
       "         [0.5279657 ],\n",
       "         [0.481069  ],\n",
       "         [0.50056535]],\n",
       "\n",
       "        [[0.49286598],\n",
       "         [0.48481622],\n",
       "         [0.4982475 ],\n",
       "         ...,\n",
       "         [0.49501196],\n",
       "         [0.51102483],\n",
       "         [0.5103969 ]]],\n",
       "\n",
       "\n",
       "       [[[0.49113482],\n",
       "         [0.51628876],\n",
       "         [0.5095932 ],\n",
       "         ...,\n",
       "         [0.49826556],\n",
       "         [0.5255077 ],\n",
       "         [0.49593028]],\n",
       "\n",
       "        [[0.48240453],\n",
       "         [0.48405367],\n",
       "         [0.5249883 ],\n",
       "         ...,\n",
       "         [0.49634966],\n",
       "         [0.48732597],\n",
       "         [0.52826023]],\n",
       "\n",
       "        [[0.49087235],\n",
       "         [0.5084052 ],\n",
       "         [0.5066332 ],\n",
       "         ...,\n",
       "         [0.5139152 ],\n",
       "         [0.4957209 ],\n",
       "         [0.5109636 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49370244],\n",
       "         [0.49465975],\n",
       "         [0.516871  ],\n",
       "         ...,\n",
       "         [0.46976066],\n",
       "         [0.5097732 ],\n",
       "         [0.46363354]],\n",
       "\n",
       "        [[0.5263436 ],\n",
       "         [0.49358448],\n",
       "         [0.54647464],\n",
       "         ...,\n",
       "         [0.5077979 ],\n",
       "         [0.48343384],\n",
       "         [0.49360576]],\n",
       "\n",
       "        [[0.493351  ],\n",
       "         [0.463913  ],\n",
       "         [0.5120881 ],\n",
       "         ...,\n",
       "         [0.4687316 ],\n",
       "         [0.50909024],\n",
       "         [0.5095418 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=443, shape=(), dtype=float32, numpy=0.16950332>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f64d713a20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.imshow(x_train[3].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "    lh = tfd.Bernoulli(probs=y_pred)\n",
    "    return - K.sum(lh.log_prob(y_true), axis=(-1, -2, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "mean loss = tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-571e475e0e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     return distribute_ctx.get_replica_context().merge_call(\n\u001b[1;32m--> 406\u001b[1;33m         self._distributed_apply, args=(grads_and_vars,), kwargs={\"name\": name})\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   1381\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1383\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    438\u001b[0m           update_ops.extend(\n\u001b[0;32m    439\u001b[0m               distribution.extended.update(\n\u001b[1;32m--> 440\u001b[1;33m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[0;32m    441\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   1544\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1546\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1553\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    423\u001b[0m         return self._resource_apply_sparse_duplicate_indices(\n\u001b[0;32m    424\u001b[0m             grad.values, var, grad.indices)\n\u001b[1;32m--> 425\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var)\u001b[0m\n\u001b[0;32m    187\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m           use_locking=self._use_locking)\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vhat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\training\\gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[1;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[1;34m\"ResourceApplyAdam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m         \u001b[0mbeta1_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1469\u001b[1;33m         \"use_locking\", use_locking, \"use_nesterov\", use_nesterov)\n\u001b[0m\u001b[0;32m   1470\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(300):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    loss_metric.reset_states()\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = nll(x_batch_train, vae(x_batch_train))\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss and priors\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "    print('mean loss =', loss_metric.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='adam', loss=nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute '_feed_output_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-de1066ba2f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m vae.fit(\n\u001b[0;32m      2\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )#validation_data=(x_test, y_test))\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2599\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2600\u001b[1;33m         \u001b[0mfeed_output_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_output_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2601\u001b[0m         \u001b[0mfeed_output_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2602\u001b[0m         \u001b[1;31m# Sample weighting not supported in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VAE' object has no attribute '_feed_output_names'"
     ]
    }
   ],
   "source": [
    "vae.fit(\n",
    "    x_train, x_train, \n",
    "    shuffle = True, epochs=10, \n",
    ")#validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vae.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "mean loss = tf.Tensor(0.7864496, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "mean loss = tf.Tensor(0.78644305, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "mean loss = tf.Tensor(0.78644305, shape=(), dtype=float32)\n",
      "Start of epoch 3\n",
      "mean loss = tf.Tensor(0.78644305, shape=(), dtype=float32)\n",
      "Start of epoch 4\n",
      "mean loss = tf.Tensor(0.786443, shape=(), dtype=float32)\n",
      "Start of epoch 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1114\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: Expecting int64_t value for attr strides, got numpy.int32",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-5fd51786afeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_batch_train\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Compute reconstruction loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmse_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-57ef20758c82>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         kl_batch = - 1/2 *  K.sum(\n\u001b[0;32m     11\u001b[0m             1 + log_var - mu**2 - K.exp(log_var), axis=-1)\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    868\u001b[0m                                 ' implement a `call` method.')\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         name=self.name)\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   1949\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1950\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1951\u001b[1;33m                            name=name)\n\u001b[0m\u001b[0;32m   1952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   1121\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[0;32m   1217\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0;32m   1218\u001b[0m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m-> 1219\u001b[1;33m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m   1220\u001b[0m   _execute.record_gradient(\n\u001b[0;32m   1221\u001b[0m       \"Conv2D\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(300):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    loss_metric.reset_states()\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "    print('mean loss =', loss_metric.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c31e3f59e8>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = decoder(tfd.Normal(0, 1).sample([1, 2])).numpy().reshape([28, 28])\n",
    "pl.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute '_feed_output_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-8602a5a0d7fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2599\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2600\u001b[1;33m         \u001b[0mfeed_output_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_output_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2601\u001b[0m         \u001b[0mfeed_output_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2602\u001b[0m         \u001b[1;31m# Sample weighting not supported in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VAE' object has no attribute '_feed_output_names'"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae._is_graph_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fb0eaf3d30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\matplotlib\\image.py:395: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = (np.float64(self.norm.vmax) -\n",
      "D:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\matplotlib\\image.py:396: UserWarning: Warning: converting a masked element to nan.\n",
      "  np.float64(self.norm.vmin))\n",
      "D:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\matplotlib\\image.py:403: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "D:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\matplotlib\\image.py:408: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "D:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\matplotlib\\colors.py:902: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "D:\\python_projects\\workon_bayes\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\numpy\\ma\\core.py:713: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "\n",
    "# linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian to produce values\n",
    "# of the latent variables z, since the prior of the latent space\n",
    "# is Gaussian\n",
    "u_grid = np.dstack(np.meshgrid(np.linspace(0.05, 0.95, n),\n",
    "                               np.linspace(0.05, 0.95, n)))\n",
    "z_grid = norm.ppf(u_grid)\n",
    "x_decoded = decoder.predict(z_grid.reshape(n*n, 2))\n",
    "x_decoded = x_decoded.reshape(n, n, digit_size, digit_size)\n",
    "\n",
    "pl.figure(figsize=(10, 10))\n",
    "pl.imshow(np.block(list(map(list, x_decoded))), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
